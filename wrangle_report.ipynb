{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this report, I will provide a summary of the gathering steps that I followed to prepare data for analysis in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We needed to gather data about the Twitter account \"WeRateDogs\" that rates dogs in a funny way. To do that, we started from three different resources: a given TSV file, another TSV file that should have been automatically uploaded, and the Twitter API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. The given Tsv file 'twitter_archive_enhanced.csv ' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just needed to download the file, upload it to our environment, and read it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Download the file  'image_predictions.tsv' automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import this file automatically, we needed to use the special Python library 'requests.' Then, we stored the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. The twitter Api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was the hard part. We first needed to create a Twitter developer account and wait for the approval of our request, which granted us elevated access. Then, we used the Tweepy library to fetch data from the API, and finally stored it as JSON data in a text file then we create from it a datafram and we store it as Tsv file to avoid use the api again (it take at least 30 minutes) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Assessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the data I gathered, I visited the Twitter account and watched some videos about the topic to collect information and gain enough background to assess logically. Then, I displayed the data in Excel sheets to visualize it easily. Of course, that was not enough, so I used what we've learned in the classroom and benefited from methods such as info(), describe(), and value_counts() to assess the data automatically. As a result, I finally managed to extract 3 tidiness issues and 8 quality issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started the cleaning process by making copies of the data to keep the original data safe. First, I solved the tidiness issues, and the most important was to merge all three datasets into one because all their data was related. Then, I continued cleaning the quality process using the skills I've learned in the classroom, like dropping columns and changing column data types. However, I didn't manage to solve some problems due to a lack of data. But in total, I ended up with a ready dataset to be visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
